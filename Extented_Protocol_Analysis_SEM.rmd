---
title: "Data analysis for the extension protocol of study: 
The interplay of cognitive reappraisal ability, 
socioeconomic status and mental health."
author: "Alimohammad Soufizadeh"
date: "March, 2022"
---
```{r}
pacman::p_load(pacman, tidyverse, kableExtra, psych, lavaan,
               gridExtra,knitr, readxl, papaja, report, mice, VIM)
```

# read all data
```{r}
raw_data <- read_excel("Coded_data.xlsx")
```

Change variable names
```{r}
raw_data <- rename(raw_data, 
                   "duration" = "Duration(Second)",
                   "gender" = "Sex")
```

# Handling Missing Data

# 1. drop all NA
```{r eval=FALSE, include=FALSE}
clean_data <- drop_na(raw_data)
```


OR take the HIGH ROAD to IMPUTATION
Missing data pattern
```{r}
missing.pattern<-md.pattern(raw_data)
```

# visualising missing data
```{r}
library(VIM)
mice_plot <- aggr(raw_data,numbers=TRUE, sortVars=TRUE,
                    labels=names(raw_data), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```

Percentage of missing data per row
```{r}
no_missing <- raw_data
percent_missing <- function(x){sum(is.na(x)/length(x)*100)}

missing <- apply(no_missing,1,percent_missing)
table(missing)
```

Rows to be imputed
```{r}
replace_rows <- subset(no_missing, missing <= 5)
no_rows <- subset(no_missing, missing > 5)
```

Missing columns
```{r}
missing <- apply(replace_rows, 2, percent_missing)
table(missing)

replace_columns <- replace_rows %>% select_if(missing<=5)
no_columns <- replace_rows %>% select_if(missing > 5)
```

Visualise missing data AGAIN!!!

```{r include=FALSE}
library(VIM)
mice_plot_1 <- aggr(replace_columns,numbers=TRUE, sortVars=TRUE,
                  labels=names(replace_columns), cex.axis=1,
                  gap=1, ylab=c("Missing data","Pattern"))
```

# IMPUTE DATA
Mice easily imputes, taking into account data type.
```{r message=TRUE, include=FALSE}
tempnomiss <- mice(replace_columns)
```

Get all the imputed data
```{r}
fixed_columns <- complete(tempnomiss)
```

Putting the data we dropped before back together with the rest
```{r}
all_columns <- cbind(no_columns, fixed_columns)
#all_rows <- rbind(all_columns, no_rows) # Only use if you want all other rows with NA binded as well
```

3. ALT way of handling Missing Data: Drop ALL NA skipping the Age column
```{r eval=FALSE, include=FALSE}
clean_data <- raw_data[complete.cases(raw_data[4:69]),]
```

# Prepare for outlier detection

```{r}
columns = c("Age","gender","id","consent", "Edu", "Platform", "duration")
demographics <- subset(all_columns, select = columns) 
all_columns <- all_columns[ , ! names(all_columns) %in% columns]
clean_data <- cbind(all_columns, demographics)
```

```{r}
#Score questionnaires:
function_score_data <-function(data){
# Create MCQ subscale variable groups
mcq_pos <- c("MCQ1","MCQ7","MCQ10",
             "MCQ19","MCQ23","MCQ28")

mcq_neg <- c("MCQ2","MCQ4","MCQ9",
             "MCQ11","MCQ15","MCQ21")

mcq_cc <- c("MCQ8","MCQ14","MCQ17",
            "MCQ24","MCQ26","MCQ29")

mcq_nc <- c("MCQ6","MCQ13","MCQ20",
            "MCQ22","MCQ25","MCQ27")

mcq_csc <- c("MCQ3","MCQ5","MCQ12",
             "MCQ16","MCQ18","MCQ30")


#save variables in separate data frame
scored_data <- no_outliers %>%
  
  #row mean of the selected columns
  mutate(across(c("PSS2", "PSS3"), ~{6 - .}), # recode PSS items 2 & 3 (6 - response)
         across(c("BSM1", "BSM3", "BSM7", "BSM8"), ~{8 - .}), # first recode BSM items 1, 3, 7, 8 (8 - response)
         mean_cra = rowMeans(select(., starts_with("CRA"))),
         mean_hcru = rowMeans(select(., starts_with("HCRU"))),
         mean_pss = rowMeans(select(., starts_with("PSS"))),
         
         mean_bsm = rowMeans(select(., starts_with("BSM"))),
         #for cesd, we need the sum
         sum_cesd = rowSums(select(., starts_with("CES_D"))),
         
         # sum all MCQ subscales
         sum_pos = rowSums(select(.,all_of(mcq_pos))),
         sum_neg = rowSums(select(.,all_of(mcq_neg))),
         sum_cc = rowSums(select(.,all_of(mcq_cc))),
         sum_nc = rowSums(select(.,all_of(mcq_nc))),
         sum_csc = rowSums(select(.,all_of(mcq_csc))),
         sum_MCQ = rowSums(select(.,starts_with("MCQ"))))%>%
  
  
  # select the scores/final variables used, remove the raw items
  select(id, Age, gender, SES,mean_cra, mean_hcru,
         sum_cesd, mean_pss, mean_bsm, sum_pos, sum_neg, sum_cc, sum_nc, sum_csc, sum_MCQ)
return(scored_data)
}
```

# Outliers

## Outliers Mahalanobis 
degrees of freedom in this case is based on the nr of variables (columns)
```{r} 
# Mahalanobis for raw data
mahal <- mahalanobis(clean_data[ , -c(63:69)],
  colMeans(clean_data[ , -c(63:69)], na.rm=TRUE),
  cov(clean_data[ , -c(63:69)], use ="pairwise.complete.obs"))

cutoff <- qchisq(p = 1 - .001, #1 minus alpha
                 df = ncol(clean_data[ , -c(63:69)])) # number of columns
```

```{r}
cutoff
summary(mahal < cutoff) #notice the direction 
clean_data <- subset(clean_data, mahal < cutoff)
```

```{r}
# Mahalanobis for scored data
scored_data <- function_score_data(clean_data) #score data first, to find outliers among scored data
mahal <- mahalanobis(scored_data[ , -c(1:3)],
  colMeans(scored_data[ , -c(1:3)], na.rm=TRUE),
  cov(scored_data[ , -c(1:3)], use ="pairwise.complete.obs"), tol = 1e-20)

cutoff <- qchisq(p = 1 - .001, #1 minus alpha
                 df = ncol(scored_data[ , -c(1:3)])) # number of columns
```

```{r}
# Exclusing outliers among scored data based on Mahalanobis
cutoff
summary(mahal < cutoff) #notice the direction 
clean_data <- subset(scored_data, mahal < cutoff)
```

## Assumptions Additivity

- Additivity is the assumption that each variable adds something to the model
- You basically do not want to use the same variable twice, as that lowers power
- Often this is described as multicollinearity (The problem is multicollineartiy). The assumption is called additivity.
- Mainly, SEM analysis has a lot of correlated variables, you just want to make sure they aren't perfectly correlated

## Assumptions Additivity

```{r}
library(corrplot)
corrplot(cor(clean_data[ , -c(1:3)]))
```

## Assumptions Set Up

```{r}
random_variable <- rchisq(nrow(clean_data), 7)
fake_model <- lm(random_variable ~ ., # What's the difference if I regress on clean_data$sum_cesd ??
                 data = clean_data[ , -c(1:3)])
standardized <- rstudent(fake_model) # residuals
fitvalues <- scale(fake_model$fitted.values) # z-score them
```

## Assumptions Linearity

- We assume the the multivariate relationship between continuous variables is linear (i.e., no curved)
- There are many ways to test this, but we can use a QQ/PP Plot to examine for linearity

```{r}
plot(fake_model, 2)
```
## Assumptions Normality

- We expect that the residuals are normally distributed
- Not that the *sample* is normally distributed 
- Generally, SEM requires a large sample size, thus, buffering against normality deviations

```{r}
hist(standardized)
```
## Assumptions Homogeneity + Homoscedasticity

- These assumptions are about equality of the variances
- We assume equal variances between groups for things like t-tests, ANOVA
- Here the assumption is equality in the spread of variance across predicted values 

```{r}
{plot(standardized, fitvalues)
  abline(v = 0)
  abline(h = 0)
}
```

# Check for duplicates
```{r}
sum(duplicated(clean_data))
```
# Data summary
```{r}
summary(clean_data)
```

# Check demographics
```{r}
clean_data$gender <- factor(clean_data$gender, levels=c(1,2,3,4))
summary (clean_data$gender)
```


# Descriptives

# In the first step, the data are summarized to get the descriptive statistics.
#Subsequently, the data are reformatted.
```{r}
descriptives <- scored_data %>% 
  dplyr::summarize(across(c(SES, Age, mean_cra, mean_hcru,sum_cesd, 
                            mean_pss, mean_bsm, sum_pos, 
                            sum_neg, sum_cc, sum_nc, sum_csc, sum_MCQ),
                          list(mean = mean, sd = sd, min = min, max = max))) %>%
  
  # bring everything in long format
  pivot_longer(everything(), names_to = "name") %>%
  
  # separate names at last underscore
  separate(name, into = c("name","descriptive"), sep = "_(?=[^_]+$)") %>%
  
  # get into a bit wider format again
  pivot_wider(names_from = name, values_from = value) %>%
  
  # rename to have nicer column names
  rename(Summary = descriptive,
         CRA = mean_cra,  #
         HCRU = mean_hcru, #
         PSS = mean_pss,  #
         CESD = sum_cesd, #
         BSM = mean_bsm,  #
         POS = sum_pos,  #
         NEG = sum_neg, # 
         CC = sum_cc, #
         NC = sum_nc, #
         CSC = sum_csc, #
         MCQT = sum_MCQ) # MCQ =Meta Cognitions Questionnaire Total
```

# Calculate cronbachâ€™s alphas
# Select the items from the raw data that belong to the specific scale.
# calculate alpha and extract raw_alpha from the list the alpha function generates.
```{r}
alpha <- clean_data %>%
  dplyr::summarize(
    # Replication Block Alphas
    cra_alpha = select(.,starts_with("CRA")) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"), # extract total and then raw_alpha from list
    hcru_alpha = select(.,starts_with("HCRU")) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    cesd_alpha = select(.,starts_with("CES_D")) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    pss_alpha = select(.,starts_with("PSS")) %>% psych::alpha(check.keys=TRUE) %>%
      pluck("total", "raw_alpha"),
    
    # BSM Alphas
    BSM_alpha = select(.,starts_with("BSM")) %>% psych::alpha(check.keys=TRUE) %>%
      pluck("total", "raw_alpha"),
    
    # MCQ scale Alphas
    pos_alpha = select(.,all_of(mcq_pos)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    neg_alpha = select(.,all_of(mcq_neg)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    cc_alpha = select(.,all_of(mcq_cc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    nc_alpha = select(.,all_of(mcq_nc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    csc_alpha = select(.,all_of(mcq_csc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    mcq_alpha = select(.,all_of(mcq_csc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"))

# add alphas as extra row to the descriptives table
descriptives <- descriptives %>%
  add_row(Summary = "alpha", SES = NA, CRA = alpha$cra_alpha, HCRU = alpha$hcru_alpha,
          PSS = alpha$pss_alpha, CESD = alpha$cesd_alpha, BSM = alpha$BSM_alpha,
          POS = alpha$pos_alpha, NEG = alpha$neg_alpha, CC = alpha$cc_alpha,
          NC = alpha$nc_alpha, CSC = alpha$csc_alpha, MCQT = alpha$mcq_alpha)
```
# make it a nicely formatted table
```{r}
library(rmarkdown)
apa_table(descriptives) # is only shown when RMarkdown document is knitted
```

# Plot monthly family income
```{r}
income_plot<-hist(scored_data$SES,
                  main="Family income distribution",
                  xlab="family income category")
```

# Mean centre all IVs for regressions with interaction terms
```{r}
centre_data_to_csv <- function(x){
centred_data <- scored_data %>%
  mutate(CRA_c = scale(mean_cra, center = TRUE, scale = FALSE),
         SES_c = scale(SES, center = TRUE, scale = FALSE),
         PSS_c = scale(mean_pss, center = TRUE, scale = FALSE),
         HCRU_c = scale(mean_hcru, center = TRUE, scale = FALSE),
         BSM_c = scale(mean_bsm, center = TRUE, scale = FALSE),
         POS_c = scale(sum_pos, center = TRUE, scale = FALSE),
         NEG_c = scale(sum_neg, center = TRUE, scale = FALSE),
         CC_c = scale(sum_cc, center = TRUE, scale = FALSE),
         NC_c = scale(sum_nc, center = TRUE, scale = FALSE),
         CSC_c = scale(sum_csc, center = TRUE, scale = FALSE),
         MCQ_c = scale(sum_MCQ, center = TRUE, scale = FALSE))

# Save prepared data
write.csv(centred_data,"prepared_data.csv", row.names = FALSE)

return(centred_data)
}
```

# read new data
```{r}
# data <- read_csv("prepared_data.csv") # Read csv data from computer
# or
data <- centre_data_to_csv(scored_data)
```

##### Three-way interaction #####

# Rename variables
```{r}
data <- data %>% mutate(rename(data,
                               "Y" = "sum_cesd",
                               "X" = "CRA_c",
                               "W" = "SES_c",
                               "Z" = "BSM_c",
                               "X1" = "PSS_c"))
```
# Create interaction terms
```{r}
data <- indProd(data1, 16, 17, 20, match = TRUE, meanC = FALSE,
        residualC = FALSE, doubleMC = FALSE, namesProd = c("X.W","X.Z","W.Z","X.W.Z"))
```

# Three-way moderation model
```{r}
mod.mod.model = 
  "
Y ~ a1 *X1 + b1*X + b2*W + b3*Z + b4*X.W  + b5*X.Z + b6*W.Z + b7*X.W.Z

#Mean of centred W & Z (for use in simple slopes)
W ~ W.mean*1
Z ~ Z.mean*1

#Variance of centred W & Z (for use in simple slopes)
Z ~~ Z.c.var*Z
W ~~ W.c.var*W

# Direct effect
direct:= b1 + b4*W.mean + b5*W.mean

#Indirect effects conditional on moderator (b1 + b4*Mod1Value + b7*Mod2Value)
direct.SDL := b1 + b4 * (W.mean - sqrt(W.c.var)) + b7 * (Z.mean - sqrt(Z.c.var))
direct.SDM := b1 + b4 * (W.mean) + b7 * (Z.mean)
direct.SDH := b1 + b4 * (W.mean - sqrt(W.c.var)) + b7 * (Z.mean + sqrt(Z.c.var))
"
```

# Fit moderation model
```{r}
mod.mod.fit=sem(model= mod.mod.model,
        data=data,
        se='bootstrap',
        bootstrap=500)

# Summarise measures
summary(mod.mod.fit,fit.measures = FALSE,standardize = TRUE, rsquare = TRUE, ci = TRUE)

# Parameters table
parameterEstimates(mod.mod.fit, remove.nonfree = TRUE)
```

```{r}
# Plot model
mod.mod.plot = semPaths(mod.mod.fit, fixedStyle = 1, layout = "tree2", 
                  intercepts = T, label.scale=T, nCharNodes = 0, 
                  sizeMan2=3, sizeMan=7, asize=2, residuals = F, exoCov = F)
```

# Simple slopes
```{r}

```

# CLEAN UP #################################################

# Clear environment
```{r}
rm(list = ls()) 
```

# Clear packages
```{r}
p_unload(all)
```

# Clear console
```{r}
cat("\014")
```
