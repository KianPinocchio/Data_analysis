---
title: "Data analysis for the extension protocol of study: The interplay of cognitive reappraisal ability, socioeconomic status and mental health."
author: "Alimohammad Soufizadeh"
date: "March, 2022"
---

```{r include=FALSE}
# Load packages
pacman::p_load(pacman, tidyverse, kableExtra, psych, lavaan, corrplot, performance, MVN, ICS, dplyr,
               matrixStats, semPlot, gridExtra,knitr, readxl, papaja, report, mice, VIM, rmarkdown, janitor)
```

# read all data
```{r}
raw_data <- read_excel("Coded_data.xlsx")
```


```{r}
# Change variable names
df_adjr <- rename(raw_data, # adjust raw data frame (df_adjr) for analyses
                   "duration" = "Duration(Second)", # in seconds
                   "gender" = "Sex")

columns = c("id", "Platform", "consent","duration", "Age","gender", "Edu", "SES")
demographics <- subset(df_adjr, select = columns) 
df_adjr <- df_adjr[ , ! names(df_adjr) %in% columns]
df_adjr <- cbind(demographics, df_adjr)
```

# RAW Demographics summary
```{r}
describe(df_adjr)
```

# RAW missing data
```{r}
# visualise missing data
mice_plot <- aggr(df_adjr,numbers=TRUE, sortVars=TRUE,
                    labels=names(df_adjr), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```

# IMPUTATION
- exclude individuals with more than five percent missing data.
```{r}
# Percentage of missing data per row
temp_no_missing <- df_adjr
percent_missing <- function(x){sum(is.na(x)/length(x)*100)}
missing <- apply(temp_no_missing,1,percent_missing)
table(missing)
  
# Look for data to be imputed based on rows
replace_rows <- subset(temp_no_missing, missing <= 5)
no_rows <- subset(temp_no_missing, missing > 5)
  
# Then look for data to be imputed based on columns
missing <- apply(replace_rows[,-c(1:7)], 2, percent_missing)
table(missing)
replace_columns <- replace_rows[,-c(1:7)] %>% select_if(missing<=5)
no_columns <- replace_rows[,-c(8:69)]
```


```{r Describe data}
temp_data <- cbind(no_columns, replace_columns)
describe(temp_data[,c(1:8)])
```

```{r}
function_data_imputation <- function(replace_columns, no_columns){  #IMPUTE DATA
  tempnomiss <- mice(replace_columns, print=FALSE) #Mice easily imputes, taking into account data type.
  
  imputed_columns <- complete(tempnomiss) # Get all the imputed data
  
  clean_data <- cbind(no_columns, imputed_columns) # Putting the data we dropped before back together with the rest
  
  return(clean_data)
}
```

```{r Impute data}
# Run the data imputation function or the replication criteria for exclusion
clean_data <- function_data_imputation(replace_columns, no_columns)
```

# Imputed data summary
```{r}
describe(clean_data)
```

# Visualise missing data AGAIN!!!
```{r}
mice_plot <- aggr(clean_data,numbers=TRUE, sortVars=TRUE,
                  labels=colnames(clean_data), cex.axis=1,
                  gap=1, ylab=c("Missing data","Pattern"))
```

# Outliers
**Outliers Mahalanobis**
- Degrees of freedom in this case is based on the nr of variables (columns)
- Detect outliers based on **unscored** data
```{r} 
# Mahalanobis for unscored data
mahal <- mahalanobis(clean_data[ , -c(1:7)],
  colMeans(clean_data[ , -c(1:7)], na.rm=TRUE),
  cov(clean_data[ , -c(1:7)], use ="pairwise.complete.obs"))

cutoff <- qchisq(p = 1 - .001, #1 minus alpha
                 df = ncol(clean_data[ , -c(1:7)])) # number of columns

cutoff # mahalanobis cutoff score
```

```{r drop outliers based on MD}
# Drop all outliers detected based on *unscored* data
summary(mahal < cutoff) #notice the direction 
no_outlier <- subset(clean_data, mahal < cutoff)
```

# No outlier data summary
```{r}
describe(no_outlier)
```

```{r check for duplicates}
# Check for duplicates
sum(duplicated(no_outlier))
```

# Score data
- Prepare to score the data with no outliers
```{r}
# Create MCQ subscale variable groups
mcq_pos <- c("MCQ1","MCQ7","MCQ10",
             "MCQ19","MCQ23","MCQ28")

mcq_neg <- c("MCQ2","MCQ4","MCQ9",
             "MCQ11","MCQ15","MCQ21")

mcq_cc <- c("MCQ8","MCQ14","MCQ17",
            "MCQ24","MCQ26","MCQ29")

mcq_nc <- c("MCQ6","MCQ13","MCQ20",
            "MCQ22","MCQ25","MCQ27")

mcq_csc <- c("MCQ3","MCQ5","MCQ12",
             "MCQ16","MCQ18","MCQ30")
```


```{r Function to score data}
# Function to Score data
function_score_data <-function(no_outlier){

#save variables in separate data frame
  scored_data <- no_outlier %>%
  
  #row mean of the selected columns
  mutate(across(c("PSS2", "PSS3"), ~{6 - .}), # recode PSS items 2 & 3 (6 - response)
         across(c("BSM1", "BSM3", "BSM7", "BSM8"), ~{8 - .}), # first recode BSM items 1, 3, 7, 8 (8 - response)
         mean_cra = rowMeans(select(., starts_with("CRA"))),
         mean_hcru = rowMeans(select(., starts_with("HCRU"))),
         mean_pss = rowMeans(select(., starts_with("PSS"))),
         
         mean_bsm = rowMeans(select(., starts_with("BSM"))),
         #for cesd, we need the sum
         sum_cesd = rowSums(select(., starts_with("CES_D"))),
         
         # sum all MCQ subscales
         sum_pos = rowSums(select(.,all_of(mcq_pos))),
         sum_neg = rowSums(select(.,all_of(mcq_neg))),
         sum_cc = rowSums(select(.,all_of(mcq_cc))),
         sum_nc = rowSums(select(.,all_of(mcq_nc))),
         sum_csc = rowSums(select(.,all_of(mcq_csc))),
         sum_MCQ = rowSums(select(.,starts_with("MCQ"))))%>%
  
  return(scored_data)
}
```

```{r Run Score data FUN}
scored_data <- function_score_data(no_outlier) # Score the data with no outliers
```

```{r Save data to csv, echo=TRUE}
write.csv(scored_data, "scored_data_imputed.csv") # Save scored data in a csv file
```

# Summary of cleaned data and scored data
```{r}
describe(scored_data[,-c(8:69)]) # Missing Data & outliers checked
describeBy(scored_data[,-c(8:69)])
```

# Assumptions: Additivity
```{r}
plot_data <- cbind(scored_data[ , 8],scored_data[ , -c(1:69)])
corPlot(cor(plot_data), upper = FALSE)
```

# Assumption: Multivariate Normality
```{r}
mvntest_result <- mvn(scored_data[8:69], mvnTest = "hz")
mvntest_result$multivariateNormality
```
# Assumption: Kurtosis
```{r}
mvnorm.kur.test(scored_data[8:69], method = "simulation")
```

# Assumption: Skewness
```{r}
mvnorm.skew.test(scored_data[8:69])
```

# Frequency tables
```{r}
# Gender
gender_freq <- scored_data %>% tabyl(gender)
levels(gender_freq$gender) <- c("male","female", "non-binary", "DWA")
apa_table(gender_freq, caption = "Proportions of Gender")
```

```{r}
# Education
edu_freq <- scored_data %>% tabyl(Edu)
levels(edu_freq$Edu) <- c("Elementary or lower","Highschool Diploma", "Bachelor's", "Master's", "PhD or higher")
apa_table(edu_freq, caption = "Proportions of Education level")
```
# Descriptives
- In the first step, the data are summarized to get the descriptive statistics.
- Subsequently, the data are reformatted.
```{r}
descriptives <- scored_data %>% 
  dplyr::summarize(across(c(SES, Age, mean_cra, mean_hcru,sum_cesd, 
                            mean_pss, mean_bsm, sum_pos,sum_neg, 
                            sum_cc, sum_nc, sum_csc, sum_MCQ),
                          list(mean = mean, sd = sd, min = min, max = max))) %>%
  
  # bring everything in long format
  pivot_longer(everything(), names_to = "name") %>%
  
  # separate names at last underscore
  separate(name, into = c("name","descriptive"), sep = "_(?=[^_]+$)") %>%
  
  # get into a bit wider format again
  pivot_wider(names_from = name, values_from = value) %>%
  
  # rename to have nicer column names
  rename(Summary = descriptive,
         CRA = mean_cra,  #
         HCRU = mean_hcru, #
         PSS = mean_pss,  #
         CESD = sum_cesd, #
         BSM = mean_bsm,  #
         POS = sum_pos,  #
         NEG = sum_neg, # 
         CC = sum_cc, #
         NC = sum_nc, #
         CSC = sum_csc, #
         MCQT = sum_MCQ) # MCQ =Meta Cognitions Questionnaire Total
```

# Cronbach’s alphas
- Select the items from the *raw* or *un-scored* data that belong to the specific scale.
- calculate alpha and extract raw_alpha from the list the alpha function generates.
```{r}
# Calculate cronbach’s alphas
alpha <- scored_data %>%
  dplyr::summarize(
    # Replication Block Alphas
    cra_alpha = select(.,starts_with("CRA")) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"), # extract total and then raw_alpha from list
    hcru_alpha = select(.,starts_with("HCRU")) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    cesd_alpha = select(.,starts_with("CES_D")) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    pss_alpha = select(.,starts_with("PSS")) %>% psych::alpha(check.keys=TRUE) %>%
      pluck("total", "raw_alpha"),
    
    # BSM Alphas
    BSM_alpha = select(.,starts_with("BSM")) %>% psych::alpha(check.keys=TRUE) %>%
      pluck("total", "raw_alpha"),
    
    # MCQ scale Alphas
    pos_alpha = select(.,all_of(mcq_pos)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    neg_alpha = select(.,all_of(mcq_neg)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    cc_alpha = select(.,all_of(mcq_cc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    nc_alpha = select(.,all_of(mcq_nc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    csc_alpha = select(.,all_of(mcq_csc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"),
    mcq_alpha = select(.,all_of(mcq_csc)) %>% psych::alpha() %>%
      pluck("total", "raw_alpha"))
```


```{r}
# add alphas as extra row to the descriptives table
descriptives <- descriptives %>%
  add_row(Summary = "alpha", SES = NA, CRA = alpha$cra_alpha, HCRU = alpha$hcru_alpha,
          PSS = alpha$pss_alpha, CESD = alpha$cesd_alpha, BSM = alpha$BSM_alpha,
          POS = alpha$pos_alpha, NEG = alpha$neg_alpha, CC = alpha$cc_alpha,
          NC = alpha$nc_alpha, CSC = alpha$csc_alpha, MCQT = alpha$mcq_alpha)
```

# Descriptives table
```{r}
# Make a nicely formatted table
apa_table(descriptives) # is only shown when RMarkdown document is knitted
```


# Plot monthly family income
```{r}
income_plot<-hist(scored_data$SES,
                  main="Family income distribution",
                  xlab="family income category")
```

```{r Rename variables for convenience in modelling}
# Rename variables for convenience in modelling
analyze_data <- scored_data %>% mutate(rename(scored_data,
                                              "X" = "mean_cra",
                                              "Y" = "sum_cesd",
                                              "W" = "SES",
                                              "Z" = "mean_bsm",
                                              "COV" = "mean_pss",
                                              "M" = "sum_neg"))%>%
  select(X, Y, W, Z, COV, M)
```

# Interaction terms
```{r}
# Create interaction terms
analyze_data <- analyze_data %>% mutate(X.W = X * W,
                        X.Z = X * Z,
                        W.Z = W * Z,
                        X.W.Z = X * W * Z,
                        M.W = M * W,
                        M.Z = M * Z,
                        M.W.Z = M * W * Z)
```


# Hypotheses C1 & C2: Moderated Mediation
```{r}
# The Model
MOD.MED.model <-
"
# Mediator
M ~ a*X

# Outcome being predicted by the model
Y ~ g*COV + c1*X + c2*W + c3*Z + c4*X.W + c5*X.Z + c6*W.Z + c7*X.W.Z +
b1*M + b2*W + b3*Z + b4*M.W + b5*M.Z + b6*W.Z + b7*M.W.Z 

# Covariance: Let all variables covary

# Intercept and variance of W for conditional effects analysis
W ~ W.mean*1
W ~~ W.var * W

# Intercept and variance of Z for conditional effects analysis
Z ~ Z.mean*1
Z ~~ Z.var*Z

# Indirect effect of X on Y through M, conditional on mean values of W and Z
indirect := (a) * ( b1 + b4*W.mean + b5*Z.mean)

# Direct effect of X on Y, moderated by W and Z
direct := c1 + c4*W.mean + c5*Z.mean

# Total effect
total := direct + indirect

# Proportion of effect mediated
prop.mediated := indirect / total

# Conditional indirect effect through mediator (M)
indirect.below :=(a)*(b1+b4*(W.mean - sqrt(W.var))+ b5*(Z.mean - sqrt(Z.var)))
indirect.above :=(a)*(b1+b4*(W.mean + sqrt(W.var))+ b5*(Z.mean + sqrt(Z.var)))

# Conditional direct effect moderated by W and Z
direct.below:=c1 + c4*(W.mean - sqrt(W.var)) + c5*(Z.mean - sqrt(Z.var))
direct.above:=c1 + c4*(W.mean + sqrt(W.var)) + c5*(Z.mean + sqrt(Z.var))

# Conditional total effect
total.below := direct.below + indirect.below
total.above := direct.above + indirect.above

# Proportion of mediation under conditions of W and one SD above or below mean
prop.mediated.below := indirect.below / total.below
prop.mediated.above := indirect.above / total.above
"
```

```{r}
# Fit the model
Mod.Med.SEM <- sem(model = MOD.MED.model,
                   data = analyze_data,
                   se = "bootstrap",
                   test = "bootstrap",
                   bootstrap = 5000) # do 5000 when actually running the code
```
# Model summary
```{r}
summary(Mod.Med.SEM, fit.measures = TRUE, standardize = TRUE, rsquare = TRUE)
```
# Parameters table
```{r warning=FALSE}
parameterEstimates(Mod.Med.SEM, remove.nonfree = TRUE, standardize = TRUE, ci = TRUE, pvalue = TRUE, output = "pretty")
```
# Standardized parameters
```{r}
std_sols <- standardizedsolution(Mod.Med.SEM)
```

```{r}
std_sols[76:87,-(c(2,4))]
```
# Plot the moderated mediation model
```{r}
# Plot model
plot = semPaths(Mod.Med.SEM, fixedStyle = 1, layout = "tree", whatLabels = "est",
                  intercepts = F, label.scale=T, nCharNodes = 5, 
                  sizeMan2=3, sizeMan=6, asize=2, residuals = F, exoCov = F)
```